{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# \ud83d\udcd3 YouTube Comment Analysis Notebook\nThis notebook processes YouTube comment CSVs, classifies topics with zero-shot learning, analyzes sentiment, reranks comments, and outputs summaries."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nimport os\nimport json\nimport pandas as pd\nfrom pathlib import Path\nfrom transformers import pipeline\nfrom tqdm.auto import tqdm\nimport traceback\n\ntqdm.pandas()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "with open(\"features.json\", \"r\") as f:\n    features_dict = json.load(f)\n    features = features_dict[\"features\"]"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "zero_shot_classifier = pipeline(\n    \"zero-shot-classification\",\n    model=\"facebook/bart-large-mnli\"\n)\nsentiment_classifier = pipeline(\n    \"sentiment-analysis\",\n    model=\"tabularisai/multilingual-sentiment-analysis\"\n)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def classify_topic(texts, batch_size=4):\n    if isinstance(texts, str):\n        texts = [texts]\n    results = zero_shot_classifier(\n        texts,\n        candidate_labels=features,\n        truncation=True,\n        batch_size=batch_size\n    )\n    if isinstance(results, dict):\n        results = [results]\n    return [max(zip(r[\"scores\"], r[\"labels\"]))[1] for r in results]\n\ndef classify_sentiment(texts):\n    texts = [text.strip() for text in texts]\n    results = sentiment_classifier(texts, truncation=True, batch_size=16)\n    return [r[\"label\"] for r in results]\n\ndef chunk_apply(arr, func, chunk_size=16):\n    return sum((func(arr[i:i+chunk_size]) for i in range(0, len(arr), chunk_size)), [])\n\ndef rerank_comments_df(df):\n    df['text_length'] = df['text'].astype(str).str.len()\n    df['likes'] = pd.to_numeric(df['likes'], errors='coerce').fillna(0)\n    df['updatedAt'] = pd.to_datetime(df['updatedAt'], errors='coerce')\n\n    min_time = df['updatedAt'].min()\n    max_time = df['updatedAt'].max()\n    df['recency_score'] = (df['updatedAt'] - min_time).dt.total_seconds() / (\n        (max_time - min_time).total_seconds() + 1e-5\n    )\n    df['norm_likes'] = (df['likes'] - df['likes'].min()) / (df['likes'].max() - df['likes'].min() + 1e-5)\n    df['norm_length'] = (df['text_length'] - df['text_length'].min()) / (df['text_length'].max() - df['text_length'].min() + 1e-5)\n    df['quality_score'] = df['norm_likes'] + df['norm_length'] + df['recency_score']\n    return df.sort_values(by=['features', 'quality_score'], ascending=[True, False]) if 'features' in df.columns else df.sort_values(by='quality_score', ascending=False)\n\nsentiment_to_score = {\n    \"Very Negative\": -2,\n    \"Negative\": -1,\n    \"Neutral\": 0,\n    \"Positive\": 1,\n    \"Very Positive\": 2\n}\n\ndef senti_score(df, path, processed_data):\n    df['senti_score'] = df['sentiment'].map(sentiment_to_score)\n    out_path = path.parent / f\"{path.stem}_processed.csv\"\n    print(\"Saving to:\", out_path)\n    print(\"DataFrame shape before saving:\", df.shape)\n    df.to_csv(out_path, index=False)\n    print(f\"Saved: {out_path.name}\")\n    processed_data.append((df, path))\n\ndef summarize_feature_scores(processed_data):\n    feature_scores = []\n    for df, path in processed_data:\n        avg_scores = df.groupby('features')['senti_score'].mean()\n        for feature, score in avg_scores.items():\n            feature_scores.append({\"product\": path.stem, \"feature\": feature, \"avg_senti_score\": score})\n    summary_df = pd.DataFrame(feature_scores)\n    summary_df = summary_df.pivot(index='product', columns='feature', values='avg_senti_score').reset_index()\n    return summary_df"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Set data directory (change path as needed)\ndata_dir = Path(\"Data\").resolve()\nprint(\"Using data directory:\", data_dir)\ncsv_paths = list(data_dir.glob(\"**/*.csv\"))\nprint(\"CSV paths found:\", csv_paths)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "processed_data = []\nfor path in csv_paths:\n    print(f\"\\nProcessing {path.name}...\")\n    try:\n        df = pd.read_csv(path)\n        print(\"Loaded CSV. Columns:\", df.columns)\n\n        required_columns = {'text', 'likes', 'updatedAt'}\n        if not required_columns.issubset(df.columns):\n            print(f\"Missing columns in {path.name}: {required_columns - set(df.columns)}\")\n            continue\n\n        df = df[df['text'].notnull()]\n        df['updatedAt'] = pd.to_datetime(df['updatedAt'], errors='coerce')\n\n        print(\"Classifying topics...\")\n        df[\"features\"] = chunk_apply(df[\"text\"].tolist(), classify_topic)\n\n        print(\"Reranking...\")\n        df = rerank_comments_df(df)\n\n        print(\"Selecting top 50 comments per feature...\")\n        df = df.groupby('features').head(50).reset_index(drop=True)\n\n        print(\"Performing sentiment analysis...\")\n        df['sentiment'] = chunk_apply(df[\"text\"].tolist(), classify_sentiment)\n\n        senti_score(df, path, processed_data)\n\n    except Exception as e:\n        print(f\"Error processing {path.name}: {e}\")\n        traceback.print_exc()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "print(\"\\nGenerating pivoted feature sentiment summary...\")\nsummary_df = summarize_feature_scores(processed_data)\nsummary_out_path = data_dir / \"feature_youtube_comment_summary.csv\"\nsummary_df.to_csv(summary_out_path, index=False)\nprint(\"Feature sentiment summary saved to:\", summary_out_path)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}