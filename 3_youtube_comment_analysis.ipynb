{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ YouTube Comment Analysis Notebook\n",
        "This notebook processes YouTube comment CSVs, classifies topics with zero-shot learning, analyzes sentiment, reranks comments, and outputs summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from transformers import pipeline\n",
        "from tqdm.auto import tqdm\n",
        "import traceback\n",
        "\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "13b71080",
      "metadata": {},
      "outputs": [],
      "source": [
        "PRODUCT = \"wireless over-ear headphones\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "741e4df1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Sound Quality',\n",
              " 'Comfort',\n",
              " 'Noise Cancellation',\n",
              " 'Battery Life',\n",
              " 'Durability']"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "DATA_DIR = Path(\"session\") / PRODUCT\n",
        "META_PATH = DATA_DIR / \"stage_1.json\"\n",
        "assert META_PATH.exists(), \"Run 1_describe_product.ipynb first!\"\n",
        "\n",
        "# Load feature labels from JSON\n",
        "with open(META_PATH, \"r\") as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "features = [o for o in metadata[\"metrics\"]]\n",
        "display(features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "zero_shot_classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"facebook/bart-large-mnli\"\n",
        ")\n",
        "sentiment_classifier = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"tabularisai/multilingual-sentiment-analysis\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_topic(texts, batch_size=16):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    results = zero_shot_classifier(\n",
        "        texts,\n",
        "        candidate_labels=features,\n",
        "        truncation=True,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "    if isinstance(results, dict):\n",
        "        results = [results]\n",
        "    return [max(zip(r[\"scores\"], r[\"labels\"]))[1] for r in results]\n",
        "\n",
        "def classify_sentiment(texts):\n",
        "    texts = [text.strip() for text in texts]\n",
        "    results = sentiment_classifier(texts, truncation=True, batch_size=32)\n",
        "    return [r[\"label\"] for r in results]\n",
        "\n",
        "def chunk_apply(arr, func, chunk_size=64):\n",
        "    return sum((func(arr[i:i+chunk_size]) for i in range(0, len(arr), chunk_size)), [])\n",
        "\n",
        "def rerank_comments_df(df):\n",
        "    df['text_length'] = df['text'].astype(str).str.len()\n",
        "    df['likes'] = pd.to_numeric(df['likes'], errors='coerce').fillna(0)\n",
        "    df['updatedAt'] = pd.to_datetime(df['updatedAt'], errors='coerce')\n",
        "\n",
        "    min_time = df['updatedAt'].min()\n",
        "    max_time = df['updatedAt'].max()\n",
        "    df['recency_score'] = (df['updatedAt'] - min_time).dt.total_seconds() / (\n",
        "        (max_time - min_time).total_seconds() + 1e-5\n",
        "    )\n",
        "    df['norm_likes'] = (df['likes'] - df['likes'].min()) / (df['likes'].max() - df['likes'].min() + 1e-5)\n",
        "    df['norm_length'] = (df['text_length'] - df['text_length'].min()) / (df['text_length'].max() - df['text_length'].min() + 1e-5)\n",
        "    df['quality_score'] = df['norm_likes'] + df['norm_length'] + df['recency_score']\n",
        "    return df.sort_values(by=['features', 'quality_score'], ascending=[True, False]) if 'features' in df.columns else df.sort_values(by='quality_score', ascending=False)\n",
        "\n",
        "sentiment_to_score = {\n",
        "    \"Very Negative\": -2,\n",
        "    \"Negative\": -1,\n",
        "    \"Neutral\": 0,\n",
        "    \"Positive\": 1,\n",
        "    \"Very Positive\": 2\n",
        "}\n",
        "\n",
        "def senti_score(df, path, processed_data):\n",
        "    df['senti_score'] = df['sentiment'].map(sentiment_to_score)\n",
        "    out_path = path.parent.with_name(\"processed_comments\") / path.name\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    print(\"Saving to:\", out_path)\n",
        "    print(\"DataFrame shape before saving:\", df.shape)\n",
        "    df.to_csv(out_path, index=False)\n",
        "    print(f\"Saved: {out_path.name}\")\n",
        "    processed_data.append((df, path))\n",
        "\n",
        "def summarize_feature_scores(processed_data):\n",
        "    feature_scores = []\n",
        "    for df, path in processed_data:\n",
        "        avg_scores = df.groupby('features')['senti_score'].mean()\n",
        "        for feature, score in avg_scores.items():\n",
        "            feature_scores.append({\"product\": path.stem, \"feature\": feature, \"avg_senti_score\": score})\n",
        "    summary_df = pd.DataFrame(feature_scores)\n",
        "    summary_df = summary_df.pivot(index='product', columns='feature', values='avg_senti_score').reset_index()\n",
        "    return summary_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using data directory: session/wireless over-ear headphones/youtube/raw_comments\n",
            "CSV paths found: [PosixPath('session/wireless over-ear headphones/youtube/raw_comments/Bose_QuietComfort_Ultra_Headphones.csv'), PosixPath('session/wireless over-ear headphones/youtube/raw_comments/Sony_WH_1000XM5.csv'), PosixPath('session/wireless over-ear headphones/youtube/raw_comments/Focal_Bathys.csv'), PosixPath('session/wireless over-ear headphones/youtube/raw_comments/Anker_Soundcore_Space_One.csv'), PosixPath('session/wireless over-ear headphones/youtube/raw_comments/Apple_AirPods_Max.csv')]\n"
          ]
        }
      ],
      "source": [
        "# Set data directory (change path as needed)\n",
        "data_dir = DATA_DIR / \"youtube\" / \"raw_comments\"\n",
        "print(\"Using data directory:\", data_dir)\n",
        "csv_paths = list(data_dir.glob(\"**/*.csv\"))\n",
        "print(\"CSV paths found:\", csv_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing Bose_QuietComfort_Ultra_Headphones.csv...\n",
            "Loaded CSV. Columns: Index(['product', 'category', 'id', 'videoId', 'parentId', 'isReply',\n",
            "       'authorName', 'authorId', 'text', 'likes', 'updatedAt'],\n",
            "      dtype='object')\n",
            "Classifying topics...\n",
            "Reranking...\n",
            "Selecting top 50 comments per feature...\n",
            "Performing sentiment analysis...\n",
            "Saving to: session/wireless over-ear headphones/youtube/processed_comments/Bose_QuietComfort_Ultra_Headphones.csv\n",
            "DataFrame shape before saving: (187, 19)\n",
            "Saved: Bose_QuietComfort_Ultra_Headphones.csv\n",
            "\n",
            "Processing Sony_WH_1000XM5.csv...\n",
            "Loaded CSV. Columns: Index(['product', 'category', 'id', 'videoId', 'parentId', 'isReply',\n",
            "       'authorName', 'authorId', 'text', 'likes', 'updatedAt'],\n",
            "      dtype='object')\n",
            "Classifying topics...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reranking...\n",
            "Selecting top 50 comments per feature...\n",
            "Performing sentiment analysis...\n",
            "Saving to: session/wireless over-ear headphones/youtube/processed_comments/Sony_WH_1000XM5.csv\n",
            "DataFrame shape before saving: (191, 19)\n",
            "Saved: Sony_WH_1000XM5.csv\n",
            "\n",
            "Processing Focal_Bathys.csv...\n",
            "Loaded CSV. Columns: Index(['product', 'category', 'id', 'videoId', 'parentId', 'isReply',\n",
            "       'authorName', 'authorId', 'text', 'likes', 'updatedAt'],\n",
            "      dtype='object')\n",
            "Classifying topics...\n",
            "Reranking...\n",
            "Selecting top 50 comments per feature...\n",
            "Performing sentiment analysis...\n",
            "Saving to: session/wireless over-ear headphones/youtube/processed_comments/Focal_Bathys.csv\n",
            "DataFrame shape before saving: (200, 19)\n",
            "Saved: Focal_Bathys.csv\n",
            "\n",
            "Processing Anker_Soundcore_Space_One.csv...\n",
            "Loaded CSV. Columns: Index(['product', 'category', 'id', 'videoId', 'parentId', 'isReply',\n",
            "       'authorName', 'authorId', 'text', 'likes', 'updatedAt'],\n",
            "      dtype='object')\n",
            "Classifying topics...\n",
            "Reranking...\n",
            "Selecting top 50 comments per feature...\n",
            "Performing sentiment analysis...\n",
            "Saving to: session/wireless over-ear headphones/youtube/processed_comments/Anker_Soundcore_Space_One.csv\n",
            "DataFrame shape before saving: (185, 19)\n",
            "Saved: Anker_Soundcore_Space_One.csv\n",
            "\n",
            "Processing Apple_AirPods_Max.csv...\n",
            "Loaded CSV. Columns: Index(['product', 'category', 'id', 'videoId', 'parentId', 'isReply',\n",
            "       'authorName', 'authorId', 'text', 'likes', 'updatedAt'],\n",
            "      dtype='object')\n",
            "Classifying topics...\n",
            "Reranking...\n",
            "Selecting top 50 comments per feature...\n",
            "Performing sentiment analysis...\n",
            "Saving to: session/wireless over-ear headphones/youtube/processed_comments/Apple_AirPods_Max.csv\n",
            "DataFrame shape before saving: (184, 19)\n",
            "Saved: Apple_AirPods_Max.csv\n"
          ]
        }
      ],
      "source": [
        "processed_data = []\n",
        "for path in csv_paths:\n",
        "    print(f\"\\nProcessing {path.name}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(path)\n",
        "        print(\"Loaded CSV. Columns:\", df.columns)\n",
        "\n",
        "        required_columns = {'text', 'likes', 'updatedAt'}\n",
        "        if not required_columns.issubset(df.columns):\n",
        "            print(f\"Missing columns in {path.name}: {required_columns - set(df.columns)}\")\n",
        "            continue\n",
        "\n",
        "        df = df[df['text'].notnull()]\n",
        "        df['updatedAt'] = pd.to_datetime(df['updatedAt'], errors='coerce')\n",
        "\n",
        "        print(\"Classifying topics...\")\n",
        "        df[\"features\"] = chunk_apply(df[\"text\"].tolist(), classify_topic)\n",
        "\n",
        "        print(\"Reranking...\")\n",
        "        df = rerank_comments_df(df)\n",
        "\n",
        "        print(\"Selecting top 50 comments per feature...\")\n",
        "        df = df.groupby('features').head(50).reset_index(drop=True)\n",
        "\n",
        "        print(\"Performing sentiment analysis...\")\n",
        "        df['sentiment'] = chunk_apply(df[\"text\"].tolist(), classify_sentiment)\n",
        "\n",
        "        senti_score(df, path, processed_data)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {path.name}: {e}\")\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating pivoted feature sentiment summary...\n",
            "Feature sentiment summary saved to: session/wireless over-ear headphones/youtube/feature_youtube_comment_summary.csv\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nGenerating pivoted feature sentiment summary...\")\n",
        "summary_df = summarize_feature_scores(processed_data)\n",
        "summary_out_path = DATA_DIR / \"youtube\" / \"feature_youtube_comment_summary.csv\"\n",
        "summary_df.to_csv(summary_out_path, index=False)\n",
        "print(\"Feature sentiment summary saved to:\", summary_out_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
