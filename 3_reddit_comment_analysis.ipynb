{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a5e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Comment Analysis for Reddit (Jupyter Notebook Version)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "from tqdm.auto import tqdm\n",
    "import traceback\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb765410",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRODUCT = \"wireless over-ear headphones\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77bf2db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sound Quality',\n",
       " 'Comfort',\n",
       " 'Noise Cancellation',\n",
       " 'Battery Life',\n",
       " 'Durability']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "DATA_DIR = Path(\"session\") / PRODUCT\n",
    "META_PATH = DATA_DIR / \"stage_1.json\"\n",
    "assert META_PATH.exists(), \"Run 1_describe_product.ipynb first!\"\n",
    "\n",
    "# Load feature labels from JSON\n",
    "with open(META_PATH, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "features = [o for o in metadata[\"metrics\"]]\n",
    "display(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af6353d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize compatible transformers pipelines\n",
    "zero_shot_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\"\n",
    ")\n",
    "sentiment_classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"tabularisai/multilingual-sentiment-analysis\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6119eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classification function\n",
    "def classify_topic(texts, batch_size=16):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    results = zero_shot_classifier(\n",
    "        texts,\n",
    "        candidate_labels=features,\n",
    "        truncation=True,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    if isinstance(results, dict):\n",
    "        results = [results]\n",
    "    return [max(zip(r[\"scores\"], r[\"labels\"]))[1] for r in results]\n",
    "\n",
    "# Sentiment scoring function\n",
    "def classify_sentiment(texts):\n",
    "    texts = [text.strip() for text in texts]\n",
    "    results = sentiment_classifier(texts, truncation=True, batch_size=32)\n",
    "    return [r[\"label\"] for r in results]\n",
    "\n",
    "# Utility: Chunked application of functions\n",
    "def chunk_apply(arr, func, chunk_size=64):\n",
    "    return sum((func(arr[i:i+chunk_size]) for i in range(0, len(arr), chunk_size)), [])\n",
    "\n",
    "# Reranking function\n",
    "def rerank_comments_df(df):\n",
    "    df['comment_length'] = df['comment_body'].astype(str).str.len()\n",
    "    min_time = df['comment_created_utc'].min()\n",
    "    max_time = df['comment_created_utc'].max()\n",
    "    df['recency_score'] = (df['comment_created_utc'] - min_time) / (max_time - min_time + 1e-5)\n",
    "    df['norm_score'] = (df['comment_score'] - df['comment_score'].min()) / (df['comment_score'].max() - df['comment_score'].min() + 1e-5)\n",
    "    df['norm_length'] = (df['comment_length'] - df['comment_length'].min()) / (df['comment_length'].max() - df['comment_length'].min() + 1e-5)\n",
    "    df['quality_score'] = df['norm_score'] + df['norm_length'] + df['recency_score']\n",
    "    return df.sort_values(by=['features', 'quality_score'], ascending=[True, False]) if 'features' in df.columns else df.sort_values(by='quality_score', ascending=False)\n",
    "\n",
    "# Map sentiment labels to scores\n",
    "sentiment_to_score = {\n",
    "    \"Very Negative\": -2,\n",
    "    \"Negative\": -1,\n",
    "    \"Neutral\": 0,\n",
    "    \"Positive\": 1,\n",
    "    \"Very Positive\": 2\n",
    "}\n",
    "\n",
    "# Function to save processed data and sentiment mapping\n",
    "def senti_score(df, path, processed_data):\n",
    "    # Map sentiment to score\n",
    "    df['senti_score'] = df['sentiment'].map(sentiment_to_score)\n",
    "\n",
    "    # Save the processed data\n",
    "    out_path = path.parent.with_name(\"processed_comments\") / path.name\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Saving to:\", out_path)\n",
    "    print(\"DataFrame shape before saving:\", df.shape)\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved: {out_path.name}\")\n",
    "\n",
    "    # Append the processed data\n",
    "    processed_data.append((df, path))\n",
    "\n",
    "# Summarize feature scores for all processed data\n",
    "def summarize_feature_scores(processed_data):\n",
    "    feature_scores = []\n",
    "    for df, path in processed_data:\n",
    "        avg_scores = df.groupby('features')['senti_score'].mean()\n",
    "        for feature, score in avg_scores.items():\n",
    "            feature_scores.append({\"product\": path.stem, \"feature\": feature, \"avg_senti_score\": score})\n",
    "    summary_df = pd.DataFrame(feature_scores)\n",
    "    summary_df = summary_df.pivot(index='product', columns='feature', values='avg_senti_score').reset_index()\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7c29b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data directory: session/wireless over-ear headphones/reddit\n",
      "CSV paths found: [PosixPath('session/wireless over-ear headphones/reddit/raw_comments/Bose_QuietComfort_Ultra_Headphones_reddit_review.csv'), PosixPath('session/wireless over-ear headphones/reddit/raw_comments/Sony_WH-1000XM5_reddit_review.csv'), PosixPath('session/wireless over-ear headphones/reddit/raw_comments/Focal_Bathys_reddit_review.csv'), PosixPath('session/wireless over-ear headphones/reddit/raw_comments/Anker_Soundcore_Space_One_reddit_review.csv'), PosixPath('session/wireless over-ear headphones/reddit/raw_comments/Apple_AirPods_Max_reddit_review.csv')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set data directory path here for notebook use (instead of argparse)\n",
    "data_dir = DATA_DIR / \"reddit\"\n",
    "print(\"Using data directory:\", data_dir)\n",
    "csv_paths = list(data_dir.glob(\"**/*.csv\"))\n",
    "print(\"CSV paths found:\", csv_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f1f5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Bose_QuietComfort_Ultra_Headphones_reddit_review.csv...\n",
      "Loaded CSV. Columns: Index(['subreddit', 'post_title', 'post_url', 'comment_body', 'comment_author',\n",
      "       'comment_score', 'comment_created_utc'],\n",
      "      dtype='object')\n",
      "Classifying topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranking...\n",
      "Selecting top 50 comments per feature...\n",
      "Performing sentiment analysis...\n",
      "Saving to: session/wireless over-ear headphones/reddit/processed_comments/Bose_QuietComfort_Ultra_Headphones_reddit_review.csv\n",
      "DataFrame shape before saving: (209, 15)\n",
      "Saved: Bose_QuietComfort_Ultra_Headphones_reddit_review.csv\n",
      "\n",
      "Processing Sony_WH-1000XM5_reddit_review.csv...\n",
      "Loaded CSV. Columns: Index(['subreddit', 'post_title', 'post_url', 'comment_body', 'comment_author',\n",
      "       'comment_score', 'comment_created_utc'],\n",
      "      dtype='object')\n",
      "Classifying topics...\n",
      "Reranking...\n",
      "Selecting top 50 comments per feature...\n",
      "Performing sentiment analysis...\n",
      "Saving to: session/wireless over-ear headphones/reddit/processed_comments/Sony_WH-1000XM5_reddit_review.csv\n",
      "DataFrame shape before saving: (211, 15)\n",
      "Saved: Sony_WH-1000XM5_reddit_review.csv\n",
      "\n",
      "Processing Focal_Bathys_reddit_review.csv...\n",
      "Loaded CSV. Columns: Index(['subreddit', 'post_title', 'post_url', 'comment_body', 'comment_author',\n",
      "       'comment_score', 'comment_created_utc'],\n",
      "      dtype='object')\n",
      "Classifying topics...\n",
      "Reranking...\n",
      "Selecting top 50 comments per feature...\n",
      "Performing sentiment analysis...\n",
      "Saving to: session/wireless over-ear headphones/reddit/processed_comments/Focal_Bathys_reddit_review.csv\n",
      "DataFrame shape before saving: (159, 15)\n",
      "Saved: Focal_Bathys_reddit_review.csv\n",
      "\n",
      "Processing Anker_Soundcore_Space_One_reddit_review.csv...\n",
      "Loaded CSV. Columns: Index(['subreddit', 'post_title', 'post_url', 'comment_body', 'comment_author',\n",
      "       'comment_score', 'comment_created_utc'],\n",
      "      dtype='object')\n",
      "Classifying topics...\n",
      "Reranking...\n",
      "Selecting top 50 comments per feature...\n",
      "Performing sentiment analysis...\n",
      "Saving to: session/wireless over-ear headphones/reddit/processed_comments/Anker_Soundcore_Space_One_reddit_review.csv\n",
      "DataFrame shape before saving: (19, 15)\n",
      "Saved: Anker_Soundcore_Space_One_reddit_review.csv\n",
      "\n",
      "Processing Apple_AirPods_Max_reddit_review.csv...\n",
      "Loaded CSV. Columns: Index(['subreddit', 'post_title', 'post_url', 'comment_body', 'comment_author',\n",
      "       'comment_score', 'comment_created_utc'],\n",
      "      dtype='object')\n",
      "Classifying topics...\n",
      "Error processing Apple_AirPods_Max_reddit_review.csv: object of type 'float' has no len()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jh2xl/Code/sch-repos/AIDS-Project-2/.venv/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py\", line 186, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "StopIteration\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_32585/50851203.py\", line 15, in <module>\n",
      "    df[\"features\"] = chunk_apply(df[\"comment_body\"].tolist(), classify_topic)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_32585/2267744225.py\", line 23, in chunk_apply\n",
      "    return sum((func(arr[i:i+chunk_size]) for i in range(0, len(arr), chunk_size)), [])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_32585/2267744225.py\", line 23, in <genexpr>\n",
      "    return sum((func(arr[i:i+chunk_size]) for i in range(0, len(arr), chunk_size)), [])\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_32585/2267744225.py\", line 5, in classify_topic\n",
      "    results = zero_shot_classifier(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jh2xl/Code/sch-repos/AIDS-Project-2/.venv/lib/python3.12/site-packages/transformers/pipelines/zero_shot_classification.py\", line 206, in __call__\n",
      "    return super().__call__(sequences, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jh2xl/Code/sch-repos/AIDS-Project-2/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 1360, in __call__\n",
      "    outputs = list(final_iterator)\n",
      "              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jh2xl/Code/sch-repos/AIDS-Project-2/.venv/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jh2xl/Code/sch-repos/AIDS-Project-2/.venv/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py\", line 269, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "                           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jh2xl/Code/sch-repos/AIDS-Project-2/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jh2xl/Code/sch-repos/AIDS-Project-2/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 764, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jh2xl/Code/sch-repos/AIDS-Project-2/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 33, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jh2xl/Code/sch-repos/AIDS-Project-2/.venv/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py\", line 195, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jh2xl/Code/sch-repos/AIDS-Project-2/.venv/lib/python3.12/site-packages/transformers/pipelines/zero_shot_classification.py\", line 209, in preprocess\n",
      "    sequence_pairs, sequences = self._args_parser(inputs, candidate_labels, hypothesis_template)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jh2xl/Code/sch-repos/AIDS-Project-2/.venv/lib/python3.12/site-packages/transformers/pipelines/zero_shot_classification.py\", line 26, in __call__\n",
      "    if len(labels) == 0 or len(sequences) == 0:\n",
      "                           ^^^^^^^^^^^^^^\n",
      "TypeError: object of type 'float' has no len()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loop through each CSV file\n",
    "processed_data = []\n",
    "for path in csv_paths:\n",
    "    print(f\"\\nProcessing {path.name}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(\"Loaded CSV. Columns:\", df.columns)\n",
    "\n",
    "        required_columns = {'comment_body', 'comment_created_utc', 'comment_score'}\n",
    "        if not required_columns.issubset(df.columns):\n",
    "            print(f\"Missing columns in {path.name}: {required_columns - set(df.columns)}\")\n",
    "            continue\n",
    "\n",
    "        print(\"Classifying topics...\")\n",
    "        df[\"features\"] = chunk_apply(df[\"comment_body\"].tolist(), classify_topic)\n",
    "\n",
    "        print(\"Reranking...\")\n",
    "        df = rerank_comments_df(df)\n",
    "\n",
    "        print(\"Selecting top 50 comments per feature...\")\n",
    "        df = df.groupby('features').head(50).reset_index(drop=True)\n",
    "\n",
    "        print(\"Performing sentiment analysis...\")\n",
    "        df['sentiment'] = chunk_apply(df[\"comment_body\"].tolist(), classify_sentiment)\n",
    "\n",
    "        # Save and append processed data\n",
    "        senti_score(df, path, processed_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path.name}: {e}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d90cb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating pivoted feature sentiment summary...\n",
      "Feature sentiment summary saved to: session/wireless over-ear headphones/reddit/feature_summary.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# After processing all files, summarize results\n",
    "print(\"\\nGenerating pivoted feature sentiment summary...\")\n",
    "summary_df = summarize_feature_scores(processed_data)\n",
    "summary_out_path = data_dir / \"feature_summary.csv\"\n",
    "summary_df.to_csv(summary_out_path, index=False)\n",
    "print(\"Feature sentiment summary saved to:\", summary_out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
