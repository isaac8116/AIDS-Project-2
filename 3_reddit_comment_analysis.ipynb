{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a5e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Comment Analysis for Reddit (Jupyter Notebook Version)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "from tqdm.auto import tqdm\n",
    "import traceback\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bf2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = Path(\"session\") / PRODUCT\n",
    "META_PATH = DATA_DIR / \"stage_1.json\"\n",
    "assert META_PATH.exists(), \"Run 1_describe_product.ipynb first!\"\n",
    "\n",
    "# Load feature labels from JSON\n",
    "with open(META_PATH, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "features = [o[\"name\"] for o in metadata[\"metrics\"]]\n",
    "display(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6353d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize compatible transformers pipelines\n",
    "zero_shot_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\"\n",
    ")\n",
    "sentiment_classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"tabularisai/multilingual-sentiment-analysis\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6119eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classification function\n",
    "def classify_topic(texts, batch_size=4):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    results = zero_shot_classifier(\n",
    "        texts,\n",
    "        candidate_labels=features,\n",
    "        truncation=True,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    if isinstance(results, dict):\n",
    "        results = [results]\n",
    "    return [max(zip(r[\"scores\"], r[\"labels\"]))[1] for r in results]\n",
    "\n",
    "# Sentiment scoring function\n",
    "def classify_sentiment(texts):\n",
    "    texts = [text.strip() for text in texts]\n",
    "    results = sentiment_classifier(texts, truncation=True, batch_size=16)\n",
    "    return [r[\"label\"] for r in results]\n",
    "\n",
    "# Utility: Chunked application of functions\n",
    "def chunk_apply(arr, func, chunk_size=16):\n",
    "    return sum((func(arr[i:i+chunk_size]) for i in range(0, len(arr), chunk_size)), [])\n",
    "\n",
    "# Reranking function\n",
    "def rerank_comments_df(df):\n",
    "    df['comment_length'] = df['comment_body'].astype(str).str.len()\n",
    "    min_time = df['comment_created_utc'].min()\n",
    "    max_time = df['comment_created_utc'].max()\n",
    "    df['recency_score'] = (df['comment_created_utc'] - min_time) / (max_time - min_time + 1e-5)\n",
    "    df['norm_score'] = (df['comment_score'] - df['comment_score'].min()) / (df['comment_score'].max() - df['comment_score'].min() + 1e-5)\n",
    "    df['norm_length'] = (df['comment_length'] - df['comment_length'].min()) / (df['comment_length'].max() - df['comment_length'].min() + 1e-5)\n",
    "    df['quality_score'] = df['norm_score'] + df['norm_length'] + df['recency_score']\n",
    "    return df.sort_values(by=['features', 'quality_score'], ascending=[True, False]) if 'features' in df.columns else df.sort_values(by='quality_score', ascending=False)\n",
    "\n",
    "# Map sentiment labels to scores\n",
    "sentiment_to_score = {\n",
    "    \"Very Negative\": -2,\n",
    "    \"Negative\": -1,\n",
    "    \"Neutral\": 0,\n",
    "    \"Positive\": 1,\n",
    "    \"Very Positive\": 2\n",
    "}\n",
    "\n",
    "# Function to save processed data and sentiment mapping\n",
    "def senti_score(df, path, processed_data):\n",
    "    # Map sentiment to score\n",
    "    df['senti_score'] = df['sentiment'].map(sentiment_to_score)\n",
    "\n",
    "    # Save the processed data\n",
    "    out_path = path.parent / f\"{path.stem}_processed.csv\"\n",
    "    print(\"Saving to:\", out_path)\n",
    "    print(\"DataFrame shape before saving:\", df.shape)\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved: {out_path.name}\")\n",
    "\n",
    "    # Append the processed data\n",
    "    processed_data.append((df, path))\n",
    "\n",
    "# Summarize feature scores for all processed data\n",
    "def summarize_feature_scores(processed_data):\n",
    "    feature_scores = []\n",
    "    for df, path in processed_data:\n",
    "        avg_scores = df.groupby('features')['senti_score'].mean()\n",
    "        for feature, score in avg_scores.items():\n",
    "            feature_scores.append({\"product\": path.stem, \"feature\": feature, \"avg_senti_score\": score})\n",
    "    summary_df = pd.DataFrame(feature_scores)\n",
    "    summary_df = summary_df.pivot(index='product', columns='feature', values='avg_senti_score').reset_index()\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c29b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set data directory path here for notebook use (instead of argparse)\n",
    "data_dir = Path(\"data\").resolve()\n",
    "print(\"Using data directory:\", data_dir)\n",
    "csv_paths = list(data_dir.glob(\"**/*.csv\"))\n",
    "print(\"CSV paths found:\", csv_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f1f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop through each CSV file\n",
    "processed_data = []\n",
    "for path in csv_paths:\n",
    "    print(f\"\\nProcessing {path.name}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(\"Loaded CSV. Columns:\", df.columns)\n",
    "\n",
    "        required_columns = {'comment_body', 'comment_created_utc', 'comment_score'}\n",
    "        if not required_columns.issubset(df.columns):\n",
    "            print(f\"Missing columns in {path.name}: {required_columns - set(df.columns)}\")\n",
    "            continue\n",
    "\n",
    "        print(\"Classifying topics...\")\n",
    "        df[\"features\"] = chunk_apply(df[\"comment_body\"].tolist(), classify_topic)\n",
    "\n",
    "        print(\"Reranking...\")\n",
    "        df = rerank_comments_df(df)\n",
    "\n",
    "        print(\"Selecting top 50 comments per feature...\")\n",
    "        df = df.groupby('features').head(50).reset_index(drop=True)\n",
    "\n",
    "        print(\"Performing sentiment analysis...\")\n",
    "        df['sentiment'] = chunk_apply(df[\"comment_body\"].tolist(), classify_sentiment)\n",
    "\n",
    "        # Save and append processed data\n",
    "        senti_score(df, path, processed_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path.name}: {e}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d90cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# After processing all files, summarize results\n",
    "print(\"\\nGenerating pivoted feature sentiment summary...\")\n",
    "summary_df = summarize_feature_scores(processed_data)\n",
    "summary_out_path = data_dir / \"feature_summary.csv\"\n",
    "summary_df.to_csv(summary_out_path, index=False)\n",
    "print(\"Feature sentiment summary saved to:\", summary_out_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
